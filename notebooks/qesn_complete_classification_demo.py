# ------------------------------------------------------------------------------
# # QESN Complete Classification Demo (Adaptive 90-95% Accuracy Pipeline)
#
# This notebook rebuilds the end-to-end QESN-MABe classification workflow using the 
# exported checkpoint (‚âà92.6% accuracy) and the adaptive optimisation plan described in
# `Mejorar_Precision_Modelo_Cuantico.md`.\n
# \n
# It is designed to run against the new exposure dataset and report production-grade metrics.
# ------------------------------------------------------------------------------
# ## Objetivos
#
# - Cargar el modelo exportado (`model_weights.bin`, `model_config.json`) y validar paridad.
# - Aplicar limpieza y balanceo temporal sobre el dataset de exposici√≥n.
# - Introducir ajustes f√≠sicos adaptativos (dt y energ√≠a) antes de cada ventana.
# - Evaluar accuracy, macro-F1 y matriz de confusi√≥n para las 37 clases.
# - Generar reportes listos para las demos profesionales.
# ------------------------------------------------------------------------------
# ## 1. Importaciones y Configuraci√≥n

import os
import sys
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

# Agregar el directorio ra√≠z del proyecto al PYTHONPATH
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from python.model_loader import load_inference
from python.qesn_inference import QESNInference  # compat for type hints

plt.rcParams['figure.figsize'] = (12, 7)
plt.rcParams['figure.dpi'] = 140
plt.rcParams['font.size'] = 10
sns.set_theme(style='whitegrid', palette='husl')

# ------------------------------------------------------------------------------
# ## 2. Rutas y Carga del Modelo
#
# Ajusta las variables de entorno `QESN_MODEL_DIR` y `QESN_DATASET_ROOT` para personalizar los paths.

MODEL_DIR = os.getenv('QESN_MODEL_DIR')
DATASET_ROOT = Path(os.getenv('QESN_DATASET_ROOT', 'data/mabe_dataset'))
TRACKING_DIR = DATASET_ROOT / 'tracking'
LABELS_CSV = DATASET_ROOT / 'labels.csv'

# Verificar si el dataset existe, si no, sugerir descarga
if not DATASET_ROOT.exists() or not LABELS_CSV.exists():
    print("‚ö†Ô∏è Dataset no encontrado!")
    print(f"   Directorio esperado: {DATASET_ROOT}")
    print(f"   Archivo de etiquetas: {LABELS_CSV}")
    print("\nüí° Para descargar el dataset:")
    print("   1. Ejecuta: python setup_dataset.py")
    print("   2. O descarga manualmente desde GitHub y extrae en data/")
    print("   3. O usa datos sint√©ticos: python create_demo_data.py")
    print("\nüîÑ Usando datos sint√©ticos para esta demostraci√≥n...")
    
    # Crear datos sint√©ticos como fallback
    import subprocess
    try:
        subprocess.run([sys.executable, "create_demo_data.py"], check=True)
        DATASET_ROOT = Path('data/exposure_dataset')
        TRACKING_DIR = DATASET_ROOT / 'tracking'
        LABELS_CSV = DATASET_ROOT / 'labels.csv'
        print("‚úÖ Datos sint√©ticos creados exitosamente")
    except:
        print("‚ùå Error creando datos sint√©ticos")
        sys.exit(1)

print('Model directory   :', MODEL_DIR or '(using default kaggle_model/)')
print('Dataset root      :', DATASET_ROOT)
print('Tracking parquet  :', TRACKING_DIR)
print('Labels CSV        :', LABELS_CSV)

# Cargar inferencia base
base_model = load_inference(MODEL_DIR)
print('\nModelo cargado:')
print('  grid      :', base_model.grid_width, 'x', base_model.grid_height)
print('  window    :', base_model.window_size)
print('  stride    :', base_model.stride)
print('  clases    :', len(base_model.class_names))

# ------------------------------------------------------------------------------
# ## 3. Utilidades de Carga y Limpieza de Datos
#
# Las funciones siguientes asumen el formato est√°ndar MABe: un `labels.csv` con columnas `video_id`, `frame`, `behavior` y un directorio de `parquet` con keypoints (`frame`, `track_id`, `keypoint`, `x`, `y`, `confidence`). Adapta las funciones si tu dataset tiene cambios de esquema.

MABE_BEHAVIORS = base_model.class_names
NUM_BEHAVIORS = len(MABE_BEHAVIORS)
KEYPOINTS_PER_MOUSE = 18
MICE_PER_FRAME = 4

@dataclass
class WindowSample:
    video_id: str
    start_frame: int
    end_frame: int
    keypoints: np.ndarray  # shape: (frames, mice, keypoints, 3)
    true_label: int

def load_labels(labels_csv: Path) -> pd.DataFrame:
    if not labels_csv.exists():
        raise FileNotFoundError(f'Archivo de etiquetas no encontrado: {labels_csv}')
    df = pd.read_csv(labels_csv)
    required = {'video_id', 'frame', 'behavior'}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f'Faltan columnas en labels.csv: {missing}')
    df['behavior'] = df['behavior'].astype(str)
    df = df[df['behavior'].isin(MABE_BEHAVIORS)].copy()
    return df

def load_tracking(video_id: str) -> pd.DataFrame:
    parquet_path = TRACKING_DIR / f'{video_id}.parquet'
    if not parquet_path.exists():
        raise FileNotFoundError(f'No existe el parquet de tracking: {parquet_path}')
    return pd.read_parquet(parquet_path)

def reshape_keypoints(df: pd.DataFrame) -> np.ndarray:
    required = {'frame', 'track_id', 'keypoint', 'x', 'y', 'confidence'}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f'Faltan columnas en tracking: {missing}')
    frames = int(df['frame'].max()) + 1
    kp_array = np.full((frames, MICE_PER_FRAME, KEYPOINTS_PER_MOUSE, 3), np.nan, dtype=np.float32)
    grouped = df.groupby(['frame', 'track_id', 'keypoint'])
    for (frame, track, kp), row in grouped:
        if track >= MICE_PER_FRAME or kp >= KEYPOINTS_PER_MOUSE:
            continue
        kp_array[frame, track, kp, 0] = row['x']
        kp_array[frame, track, kp, 1] = row['y']
        kp_array[frame, track, kp, 2] = row['confidence']
    return kp_array

def interpolate_missing(keypoints: np.ndarray) -> np.ndarray:
    mask = np.isnan(keypoints)
    if not mask.any():
        return keypoints
    filled = np.copy(keypoints)
    for axis in range(3):
        data = filled[..., axis]
        missing = np.isnan(data)
        if missing.any():
            idx = np.arange(data.shape[0])
            for mouse in range(data.shape[1]):
                for kp in range(data.shape[2]):
                    series = data[:, mouse, kp]
                    miss = np.isnan(series)
                    if miss.all():
                        continue
                    valid_idx = idx[~miss]
                    valid_vals = series[~miss]
                    series[miss] = np.interp(idx[miss], valid_idx, valid_vals)
                    data[:, mouse, kp] = series
    filled[..., 2] = np.clip(filled[..., 2], 0.0, 1.0)
    return filled

def compute_window_label(labels_df: pd.DataFrame, video_id: str, start: int, end: int) -> Optional[int]:
    window_labels = labels_df[(labels_df['video_id'] == video_id) & (labels_df['frame'] >= start) & (labels_df['frame'] < end)]
    if window_labels.empty:
        return None
    counts = window_labels['behavior'].value_counts()
    return MABE_BEHAVIORS.index(counts.idxmax())

def iter_windows(keypoints: np.ndarray, window: int, stride: int) -> Iterable[Tuple[int, int, np.ndarray]]:
    total_frames = keypoints.shape[0]
    for start in range(0, total_frames - window + 1, stride):
        end = start + window
        yield start, end, keypoints[start:end]

def generate_samples(video_id: str, keypoints: np.ndarray, labels_df: pd.DataFrame, window: int, stride: int) -> List[WindowSample]:
    samples: List[WindowSample] = []
    for start, end, chunk in iter_windows(keypoints, window, stride):
        label = compute_window_label(labels_df, video_id, start, end)
        if label is None:
            continue
        samples.append(WindowSample(video_id, start, end, chunk, label))
    return samples

# ------------------------------------------------------------------------------
# ## 4. Modelo Adaptativo
#
# Implementamos un contenedor que ajusta `dt`, `energy_injection` y `coupling_strength` seg√∫n la cinem√°tica de cada ventana antes de delegar en `QESNInference`.

class AdaptiveQESN:
    def __init__(self, model: QESNInference):
        self.model = model

    @staticmethod
    def _window_stats(keypoints: np.ndarray) -> Dict[str, float]:
        displacements = np.diff(keypoints[..., :2], axis=0)
        speeds = np.linalg.norm(displacements, axis=-1)
        mean_speed = np.nanmean(speeds) if speeds.size else 0.0
        max_speed = np.nanmax(speeds) if speeds.size else 0.0
        valid_ratio = np.mean(keypoints[..., 2] > 0.5)
        return {
            'mean_speed': float(np.nan_to_num(mean_speed)),
            'max_speed': float(np.nan_to_num(max_speed)),
            'valid_ratio': float(np.nan_to_num(valid_ratio))
        }

    def _configure_physics(self, stats: Dict[str, float]):
        # Configuraci√≥n m√°s diversa y menos restrictiva
        base_coupling = 0.5
        
        # Variar m√°s el coupling basado en la velocidad
        speed_factor = stats['mean_speed'] / 20.0  # Normalizar por velocidad t√≠pica
        coupling = np.clip(base_coupling + 0.1 * (speed_factor - 0.5), 0.3, 0.7)
        
        # Diffusion independiente del coupling para m√°s variabilidad
        diffusion = np.clip(0.4 + 0.1 * np.random.random(), 0.3, 0.6)
        
        # dt m√°s variable basado en velocidad m√°xima
        if stats['max_speed'] > 50.0:
            dt = 0.0010  # Movimiento muy r√°pido
        elif stats['max_speed'] > 25.0:
            dt = 0.0015  # Movimiento r√°pido
        else:
            dt = 0.0020  # Movimiento lento
        
        # Energy m√°s variable para permitir diferentes comportamientos
        base_energy = 0.05
        speed_energy_factor = 1.0 + 0.3 * (stats['mean_speed'] / 20.0)
        valid_energy_factor = 0.8 + 0.4 * stats['valid_ratio']
        energy = base_energy * speed_energy_factor * valid_energy_factor
        
        # Agregar algo de aleatoriedad para diversidad
        energy *= (0.9 + 0.2 * np.random.random())
        
        self.model.foam.set_coupling_strength(coupling)
        self.model.foam.set_diffusion_rate(diffusion)
        self.model.foam.set_decay_rate(0.001)
        self.model.dt = dt
        self.model.energy_injection = energy

    def predict_window(self, window: WindowSample, video_width: int, video_height: int):
        stats = self._window_stats(window.keypoints)
        self._configure_physics(stats)
        pred_idx, probs, pred_name = self.model.predict(window.keypoints, video_width, video_height, window_size=self.model.window_size)
        
        # Calcular estad√≠sticas adicionales para debugging
        top_probs = np.sort(probs)[-3:][::-1]  # Top 3 probabilidades
        top_indices = np.argsort(probs)[-3:][::-1]  # Top 3 √≠ndices
        
        return {
            'video_id': window.video_id,
            'start': window.start_frame,
            'end': window.end_frame,
            'pred_idx': int(pred_idx),
            'pred_name': pred_name,
            'probabilities': probs,
            'stats': stats,
            'top3_probs': top_probs,
            'top3_behaviors': [MABE_BEHAVIORS[i] for i in top_indices],
            'coupling': self.model.foam.coupling_strength,
            'diffusion': self.model.foam.diffusion_rate,
            'dt': self.model.dt,
            'energy': self.model.energy_injection
        }

adaptive_model = AdaptiveQESN(base_model)

# ------------------------------------------------------------------------------
# ## 5. Evaluaci√≥n del Dataset
#
# La funci√≥n `evaluate_dataset` recorre todos los videos, genera ventanas de 60 frames (stride 30) y calcula accuracy/macro-F1. Ajusta `max_videos` para reducir tiempo durante depuraci√≥n.

def evaluate_dataset(labels_df: pd.DataFrame, max_videos: Optional[int] = None) -> Dict[str, object]:
    if not TRACKING_DIR.exists():
        raise FileNotFoundError(f'Directorio de tracking no disponible: {TRACKING_DIR}')
    video_ids = sorted(labels_df['video_id'].unique())
    if max_videos:
        video_ids = video_ids[:max_videos]

    y_true: List[int] = []
    y_pred: List[int] = []
    records: List[Dict[str, object]] = []

    for video_id in tqdm(video_ids, desc='Evaluating videos'):
        tracking_df = load_tracking(video_id)
        keypoints = reshape_keypoints(tracking_df)
        keypoints = interpolate_missing(keypoints)
        samples = generate_samples(video_id, keypoints, labels_df, base_model.window_size, base_model.stride)
        if not samples:
            continue
        video_width = int(np.nanmax(keypoints[..., 0]))
        video_height = int(np.nanmax(keypoints[..., 1]))
        for sample in samples:
            result = adaptive_model.predict_window(sample, video_width, video_height)
            y_true.append(sample.true_label)
            y_pred.append(result['pred_idx'])
            records.append({
                'video_id': video_id,
                'start_frame': sample.start_frame,
                'end_frame': sample.end_frame,
                'true_label': sample.true_label,
                'pred_label': result['pred_idx'],
                'true_behavior': MABE_BEHAVIORS[sample.true_label],
                'pred_behavior': result['pred_name'],
                'confidence': float(result['probabilities'][result['pred_idx']]),
                'mean_speed': result['stats']['mean_speed'],
                'max_speed': result['stats']['max_speed'],
                'valid_ratio': result['stats']['valid_ratio'],
                'top3_probs': result['top3_probs'],
                'top3_behaviors': result['top3_behaviors'],
                'coupling': result['coupling'],
                'diffusion': result['diffusion'],
                'dt': result['dt'],
                'energy': result['energy']
            })

    if not y_true:
        raise RuntimeError('No se generaron ventanas con etiquetas v√°lidas.')

    accuracy = accuracy_score(y_true, y_pred)
    macro_f1 = f1_score(y_true, y_pred, average='macro')
    
    # Obtener las clases √∫nicas presentes en los datos
    unique_labels = sorted(set(y_true + y_pred))
    present_behaviors = [MABE_BEHAVIORS[i] for i in unique_labels]
    
    report = classification_report(y_true, y_pred, labels=unique_labels, target_names=present_behaviors, digits=3)
    cm = confusion_matrix(y_true, y_pred, labels=unique_labels)
    results_df = pd.DataFrame.from_records(records)

    return {
        'accuracy': accuracy,
        'macro_f1': macro_f1,
        'report': report,
        'confusion_matrix': cm,
        'predictions': results_df
    }

# ------------------------------------------------------------------------------
# ## 6. Ejecutar Evaluaci√≥n
#
# Si a√∫n no tienes el dataset preparado, ejecuta primero el pipeline de extracci√≥n. Ajusta `max_videos` para pruebas r√°pidas.

labels_df = load_labels(LABELS_CSV)
print(f'Videos disponibles: {labels_df.video_id.nunique()}')
print(labels_df.head())

# Cambia max_videos a None para procesar todo el dataset
EVAL_RESULTS = evaluate_dataset(labels_df, max_videos=None)
print(f"Accuracy: {EVAL_RESULTS['accuracy']*100:.2f}%")
print(f"Macro F1: {EVAL_RESULTS['macro_f1']*100:.2f}%")


low_conf_df = EVAL_RESULTS['predictions'][EVAL_RESULTS['predictions'].confidence < 0.10]
print(f'Bajas confianzas detectadas: {len(low_conf_df)} ventanas')
print('\nVentanas con baja confianza:')
print(low_conf_df[['video_id', 'start_frame', 'end_frame', 'true_behavior', 'pred_behavior', 'confidence', 'valid_ratio', 'mean_speed']].head())

# ------------------------------------------------------------------------------
# ## 7. Visualizaci√≥n de Resultados

pred_df = EVAL_RESULTS['predictions']
cm = EVAL_RESULTS['confusion_matrix']

# Obtener las clases presentes en los resultados
unique_labels = sorted(set(EVAL_RESULTS['predictions']['true_label'].tolist() + EVAL_RESULTS['predictions']['pred_label'].tolist()))
present_behaviors = [MABE_BEHAVIORS[i] for i in unique_labels]

fig, ax = plt.subplots(figsize=(14, 12))
sns.heatmap(cm, ax=ax, cmap='viridis', linewidths=0.1, linecolor='gray', cbar=True, square=True)
ax.set_title('Confusion Matrix - QESN Adaptive Inference')
ax.set_xlabel('Predicted')
ax.set_ylabel('True')
ax.set_xticks(np.arange(len(unique_labels)) + 0.5)
ax.set_xticklabels(present_behaviors, rotation=90)
ax.set_yticks(np.arange(len(unique_labels)) + 0.5)
ax.set_yticklabels(present_behaviors, rotation=0)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 4))
sns.histplot(pred_df['confidence'], bins=30, kde=True)
plt.title('Distribution of Prediction Confidence')
plt.xlabel('confidence')
plt.ylabel('count')
plt.tight_layout()
plt.show()

print('Classification Report:')
print()
print(EVAL_RESULTS['report'])

# An√°lisis adicional de las predicciones
print('\n' + '='*60)
print('AN√ÅLISIS DETALLADO DE PREDICCIONES')
print('='*60)

pred_df = EVAL_RESULTS['predictions']

print(f'\nDistribuci√≥n de predicciones:')
pred_counts = pred_df['pred_behavior'].value_counts()
print(pred_counts.head(10))

print(f'\nDistribuci√≥n de etiquetas verdaderas:')
true_counts = pred_df['true_behavior'].value_counts()
print(true_counts.head(10))

print(f'\nTop 3 comportamientos m√°s confusos (predicciones incorrectas):')
incorrect = pred_df[pred_df['true_behavior'] != pred_df['pred_behavior']]
if len(incorrect) > 0:
    confusion_pairs = incorrect.groupby(['true_behavior', 'pred_behavior']).size().sort_values(ascending=False)
    print(confusion_pairs.head(10))

print(f'\nEstad√≠sticas de par√°metros adaptativos:')
param_stats = pred_df[['coupling', 'diffusion', 'dt', 'energy']].describe()
print(param_stats)

print(f'\nCorrelaci√≥n entre par√°metros y confianza:')
correlations = pred_df[['coupling', 'diffusion', 'dt', 'energy', 'confidence']].corr()['confidence'].sort_values(ascending=False)
print(correlations)

# ------------------------------------------------------------------------------
# ## 8. Persistencia y Reporting
#
# Guarda los resultados para integrarlos con los demos multimedia.

OUTPUT_DIR = DATASET_ROOT / 'results'
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

pred_path = OUTPUT_DIR / 'qesn_adaptive_predictions.parquet'
report_path = OUTPUT_DIR / 'qesn_classification_report.txt'
cm_path = OUTPUT_DIR / 'qesn_confusion_matrix.npy'

pred_df.to_parquet(pred_path, index=False)
np.save(cm_path, EVAL_RESULTS['confusion_matrix'])
with open(report_path, 'w', encoding='utf-8') as f:
    f.write(f"Accuracy: {EVAL_RESULTS['accuracy']*100:.3f}%\n")
    f.write(f"Macro F1: {EVAL_RESULTS['macro_f1']*100:.3f}%\n\n")
    f.write(EVAL_RESULTS['report'])

print('Resultados guardados en:', OUTPUT_DIR)

# ------------------------------------------------------------------------------
# ## 9. Pr√≥ximos Pasos
#
# - Repetir la evaluaci√≥n con validaci√≥n cruzada (3 folds) si se requiere certificaci√≥n.
# - Ajustar los par√°metros adaptativos (`dt`, energ√≠a, coupling) seg√∫n el an√°lisis de los resultados guardados en `results/`.
# - Integrar estos artefactos en los demos (`demo_espectacular.py`, notebooks profesionales).
